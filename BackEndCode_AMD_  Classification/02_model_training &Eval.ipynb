{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0065733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "get_ipython().run_line_magic('matplotlib', 'inline') ###\n",
    "################################################################################\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "################################################################################\n",
    "from keras import layers,utils\n",
    "from keras.datasets import cifar100\n",
    "from keras.models import Sequential,load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, Activation, Flatten,Conv2D,MaxPooling2D, MaxPool2D,Add, concatenate\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,LeakyReLU, Dense, InputLayer,Dropout, Activation, Flatten,Input, BatchNormalization, Conv2D, MaxPool2D, GlobalMaxPool2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16, MobileNet, ResNet50, InceptionV3, Xception, VGG19,ResNet101\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "################################################################################\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global_Variables\n",
    "\n",
    "# Path for the dataset directory which contains directory for each class (GA , Intermediate , Normal , Wet)\n",
    "dataset_path = '../AMD_DataSet'\n",
    "\n",
    "# Path to store the splitted dataset in it as (train , val , test)\n",
    "dataset_splitted_path = '../AMD_DataSet_pre'\n",
    "train_data_path = os.path.join(dataset_splitted_path , 'train')\n",
    "val_data_path = os.path.join(dataset_splitted_path , 'val')\n",
    "test_data_path = os.path.join(dataset_splitted_path , 'test')\n",
    "# Spiltting ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Adjusting Seed Value to 777\n",
    "seed_constant = 777\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)\n",
    "\n",
    "# Variables for Augmentation Generators\n",
    "dataset_augmented_path = '../AMD_DataSet_aug'\n",
    "target_size = (448, 448)\n",
    "batch_size = 64\n",
    "\n",
    "# Variables for Compilation\n",
    "OPTIMIZER = SGD(learning_rate=0.001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model_name=\"AMD_SA_ResNet50_TF_FINAL\"\n",
    "\n",
    "# Variables for fitting\n",
    "epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import splitfolders\n",
    "\n",
    "# Function to split the data to train , val , test\n",
    "def splitting_data_to_train_test_val(dataset_path , output_path , train_ratio , val_ratio , test_ratio , seed_value = 777):\n",
    "    try:\n",
    "        # This will include loading and splitting dataset\n",
    "        splitfolders.ratio(dataset_path, output=output_path, seed=seed_value, ratio=(train_ratio, val_ratio, test_ratio))\n",
    "        print(f\"(✔) Splitting data is completed, with ratio of {train_ratio} for training ; {val_ratio} for validation ; {test_ratio} for testing.\")\n",
    "    except :\n",
    "        print(f'(X) Error while splitting data.')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Function to augment data (train , val) before training\n",
    "def augmenting_data(train_data_path , val_data_path , output_path , target_size , batch_size , seed_value = 777):\n",
    "    try:\n",
    "        # Data Generators\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255.,\n",
    "            horizontal_flip = True,\n",
    "            vertical_flip = True,\n",
    "            rotation_range = 50)\n",
    "        \n",
    "        val_datagen   = ImageDataGenerator(\n",
    "            rescale=1./255)\n",
    "        \n",
    "        # Augmenting Data\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            train_data_path,\n",
    "            seed=seed_value,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode = 'categorical')\n",
    "\n",
    "        val_generator = val_datagen.flow_from_directory(\n",
    "            val_data_path,\n",
    "            seed=seed_value,\n",
    "            target_size=target_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode = 'categorical')\n",
    "        \n",
    "        print(f\"(✔) Augmenting data is Successfully done.\")\n",
    "    except :\n",
    "        print(f'(X) Error while Augmenting data.')\n",
    "        return None\n",
    "    return train_generator , val_generator\n",
    "\n",
    "# Function to evaluate the steps for the generator:\n",
    "def evaluate_steps(generator):\n",
    "    try:\n",
    "        steps = generator.n // generator.batch_size\n",
    "        print(f\"(✔) Steps for the generator is evaluated {steps} steps. \")\n",
    "    except :\n",
    "        print(f'(X) Error while evaluating the generator steps.')\n",
    "        return None\n",
    "    return steps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6af96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_data_to_train_test_val(dataset_path , dataset_splitted_path , train_ratio , val_ratio , test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910843cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator , val_generator = augmenting_data(train_data_path , val_data_path , dataset_augmented_path, target_size , batch_size , seed_constant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aecd5b",
   "metadata": {},
   "source": [
    "## Snippet code integrated from 01_model_building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe5c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global_Variables\n",
    "inputs = Input(shape=(448, 448, 3))\n",
    "sa_model_path = './models/MSE_MSLE_model001.h5'\n",
    "pretrained_model = ResNet50(input_shape=(224, 224, 3),weights='imagenet', include_top=False)\n",
    "final_model_name = 'AMD_SA_ResNet50_TF_FINAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf410ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some defined Loss Functions used by the Scale Adaptive model (Encoder model)\n",
    "def rmse(y_true, y_pred):\n",
    "  return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def custom_metric_SSIM_(y_true, y_pred):\n",
    "        x=y_true\n",
    "        y=y_pred\n",
    "        mean_x=tf.math.reduce_mean(x)\n",
    "        mean_y=tf.math.reduce_mean(y)\n",
    "\n",
    "        var_x=tf.math.reduce_variance(x)\n",
    "        var_y=tf.math.reduce_variance(y)\n",
    "        covar_xy=tfp.stats.covariance(x,y)\n",
    "\n",
    "        k1=0.01\n",
    "        k2=0.03\n",
    "        l=float(tf.math.pow(2,20) - 1)\n",
    "        c1=float(k1 * l)\n",
    "        c2=float(k2 * l)\n",
    "        print(type(c1))\n",
    "        c1 = tf.math.square(c1)\n",
    "        c2 = tf.math.square(c2)\n",
    "      # SSIM=(2μxμy + c1)(2σxy +c2)/((μx)2+(μy)2 +c1)((σx)2 +(σy)2 + c2)\n",
    "        m=tf.math.multiply(2.0,covar_xy)\n",
    "        t1=tf.math.add(m , c2)\n",
    "        m=tf.math.multiply(mean_x,mean_y)\n",
    "        m=float(m * 2.0)\n",
    "        t2=tf.math.add(m ,c1)\n",
    "        t3=(tf.math.square(mean_x)+tf.math.square(mean_y) +c1)\n",
    "        t4=(tf.math.square(var_x) +tf.math.square(var_y) + c2)\n",
    "\n",
    "        SSIM=tf.math.divide(tf.math.multiply(t2,t1),tf.math.multiply(t3,t4))\n",
    "        return tf.math.reduce_mean(SSIM)\n",
    "\n",
    "def custom_loss_MSEhigh_MSLElow_(y, y_pred):\n",
    "        l1=tf.keras.losses.MeanSquaredError()\n",
    "        l_high=l1(y[:,:,:,:12],y_pred[:,:,:,:12])\n",
    "\n",
    "        l2=tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "        l_low=l2(y[:,:,:,12:],y_pred[:,:,:,12:])\n",
    "\n",
    "        return 0.4 * l_high + 0.6 * l_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747864e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the encoder model or Scale Adaptive model\n",
    "def load_encoder_model(model_path , inputs):\n",
    "    try:\n",
    "        resized_model = tf.keras.models.load_model(\n",
    "            model_path,\n",
    "            custom_objects={\"custom_metric_SSIM_\":custom_metric_SSIM_,\n",
    "                            \"custom_loss_MSEhigh_MSLElow_\":custom_loss_MSEhigh_MSLElow_,\n",
    "                            \"rmse\":rmse},\n",
    "            compile=False)\n",
    "        resized_model.trainable = False\n",
    "        print(f\"(✔) Model with name of '{model_path.split('/')[-1]}' is Successfuly Loaded....\")\n",
    "    except:\n",
    "        print(f'(X) Cannot find the model in this path {model_path}')\n",
    "        return None\n",
    "    return resized_model(inputs) \n",
    "\n",
    "# Function to join the pretrained_model with the Scale Adaptive model for feature extraction\n",
    "def build_feature_extractor_model(inputs , pretrained_model):\n",
    "    # inputs here is the Scale Adaptive model\n",
    "    try:\n",
    "        feature_extractor_model = pretrained_model\n",
    "        for layer in feature_extractor_model.layers:\n",
    "            layer.trainable = True\n",
    "        print(f\"(✔) Successfully loaded the pretrained model for transfer learning and combine with the Scale Adaptive model.....\")\n",
    "    except:\n",
    "        print(f'(X) Error while loading the pretrained model....')\n",
    "        return None\n",
    "    return feature_extractor_model(inputs)\n",
    "\n",
    "# Function to attach the classification layers to classify our data\n",
    "def attach_classifier(inputs):\n",
    "    # inputs here is the feature extractor model which is (SA+ResNET50) so far\n",
    "    try:\n",
    "        X = GlobalAveragePooling2D()(inputs)\n",
    "        X = Flatten()(X)\n",
    "\n",
    "        X = Dense(units=512, activation='relu')(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = Dropout(0.2)(X)\n",
    "\n",
    "        X = Dense(units=256, activation='relu')(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = Dropout(0.2)(X)\n",
    "\n",
    "        X = Dense(units=128, activation='relu')(X)\n",
    "        X = BatchNormalization()(X)\n",
    "        X = Dropout(0.2)(X)\n",
    "\n",
    "        X = Dense(units=4, activation='softmax', name=final_model_name)(X)\n",
    "        print(f\"(✔) Successfully attached the classification layers....\")\n",
    "    except:\n",
    "        print(f'(X) Error while attaching the classification layers....')\n",
    "        return None\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19eb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_model(sa_model_path , inputs ,pretrained_model):\n",
    "    try:\n",
    "        resized_model = load_encoder_model(sa_model_path,inputs)\n",
    "        resized_pretrained_model = build_feature_extractor_model(inputs= resized_model[:,:,:,12:] , pretrained_model= pretrained_model)\n",
    "        resized_pretrained_classification_model = attach_classifier(inputs= resized_pretrained_model)\n",
    "        final_model = tf.keras.Model(inputs=inputs, outputs = resized_pretrained_classification_model)\n",
    "        print(f\"(✔) Final Model is Successfully Built....\")\n",
    "    except:\n",
    "        print(f'(X) Error while building the model....')\n",
    "        return None\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a15b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = building_model(sa_model_path , inputs ,pretrained_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6452f",
   "metadata": {},
   "source": [
    "## Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compile the model\n",
    "def compiling_model(model , optimizer , loss):\n",
    "    try:\n",
    "        model.compile(optimizer = optimizer,#'SGD',\n",
    "                  loss = loss,\n",
    "                  metrics = ['accuracy',tf.keras.metrics.AUC(),\n",
    "                             tf.keras.metrics.Recall(), tf.keras.metrics.Precision(),\n",
    "                             tf.keras.metrics.TruePositives(),tf.keras.metrics.TrueNegatives(),\n",
    "                             tf.keras.metrics.FalsePositives(),tf.keras.metrics.FalseNegatives()])\n",
    "        print(f\"(✔) Adjusting compilaion step is completed....\")\n",
    "    except:\n",
    "        print(f'(X) Error while compiling the model....')\n",
    "        return None\n",
    "    return model\n",
    "\n",
    "# Function to fit the model \"start training\"\n",
    "def fitting_model(model , epochs , batch_size , train_generator , val_generator):\n",
    "    try:\n",
    "        # Setting up some callback functions\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=50,verbose=1,factor=0.2)\n",
    "        checkpoint = ModelCheckpoint('models/'+final_model_name+'.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "        early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)##\n",
    "        \n",
    "        # Evaluating Training and Evaluation Steps\n",
    "        train_steps = evaluate_steps(train_generator)\n",
    "        validation_steps = evaluate_steps(val_generator)\n",
    "        \n",
    "        # Start training or fitting model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch= train_steps,\n",
    "            epochs=epochs,\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            verbose = 1,\n",
    "            callbacks=[reduce_lr, early_stop, checkpoint])\n",
    "\n",
    "        print(f\"(✔) Model is Trained Successfully....\")\n",
    "    except:\n",
    "        print(f'(X) Error while training the model....')\n",
    "        return None\n",
    "    return model , history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f08564",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = compiling_model(model , OPTIMIZER , loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model , history = fitting_model(model , epochs , batch_size , train_generator , val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961cac2",
   "metadata": {},
   "source": [
    "## Evaluating Model using \"Testing Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f81bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model using the testing data\n",
    "def evaluating_model(model , val_generator , test_generator):\n",
    "    try:\n",
    "        # Evaluating Model\n",
    "        valid_loss, valid_accuracy, valid_auc, valid_recall, valid_precision, valid_true_positives, valid_true_negatives, valid_false_positives, valid_false_negatives = model.evaluate(val_generator)\n",
    "        test_loss, test_accuracy, test_auc, test_recall, test_precision, test_true_positives, test_true_negatives, test_false_positives, test_false_negatives   = model.evaluate(test_generator)\n",
    "\n",
    "        print('Validation Accuracy: ', round(valid_accuracy * 100, 1), \"%\")\n",
    "        print('Test Accuracy: ', round(test_accuracy * 100, 1), \"%\")\n",
    "        print(\" \")\n",
    "        print('Validation Specifity: ', round(valid_true_negatives / (valid_true_negatives + valid_false_positives) * 100, 1), \"%\")\n",
    "        print('Test Specifity: ', round(test_true_negatives / (test_true_negatives + test_false_positives) * 100, 1), \"%\")\n",
    "        print(\" \")\n",
    "        print('Validation sensitivity: ', round(valid_true_positives / (valid_true_positives+valid_false_negatives) * 100, 1), \"%\")\n",
    "        print('Test sensitivity: ', round(test_true_positives / (test_true_positives + test_false_negatives) * 100, 1), \"%\")\n",
    "        print(\" \")\n",
    "        print('Validation F1-score: ', round(2 * valid_true_positives / ((2 * valid_true_positives) + valid_false_positives + valid_false_negatives) * 100, 1), \"%\")\n",
    "        print('Test F1-score: ', round(2 * test_true_positives / ((2 * test_true_positives) + test_false_positives + test_false_negatives) * 100, 1), \"%\")\n",
    "        print(\" \")\n",
    "        print('Validation Loss: ', round(valid_loss, 1))\n",
    "        print('Test Loss: ', round(test_loss, 1))\n",
    "        print(f\"(✔) Evaluation step is completed....\")\n",
    "    except:\n",
    "        print(f'(X) Error while Evaluation the model....')\n",
    "        return None\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fdc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator for test data\n",
    "test_datagen   = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_path,\n",
    "    seed=seed_constant,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "evaluating_model(model , val_generator , test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c3327f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
